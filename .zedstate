{"split":"2-0","session.current":["/learn_wmpi.py","/rlearning.py"],"session.open":{"/rlearning.py":{"scrollTop":4262.5,"scrollLeft":0,"selection":{"start":{"row":257,"column":30},"end":{"row":257,"column":30}},"lastUse":1540942553745,"undo":[[{"group":"doc","deltas":[{"action":"removeLines","range":{"start":{"row":0,"column":0},"end":{"row":384,"column":0}},"nl":"\n","lines":["import math, time, random","import numpy as np","import tensorflow as tf","","from log_utils import *","","def rewards_to_qvals(t_r_l, gamma):","  T = t_r_l.shape[0]","  # reward = average of all following rewards","  # for t in range(T):","  #   t_r_l[t, 0] = np.mean(t_r_l[t:, 0])","  ","  # for t in range(T):","  #   cumw, cumr = 0, 0","  #   for i, r in enumerate(t_r_l[t:, 0] ):","  #     cumw += gamma**i","  #     cumr += gamma**i * r","  #   t_r_l[t, 0] = cumr/cumw","  # return t_r_l","  ","  t_dr_l = np.zeros((T, 1))","  cumw, cumr = 0, 0","  for t in range(T-1, -1, -1):","    cumr = t_r_l[t, 0] + gamma*cumr","    # cumw = 1 + gamma*cumw","    # t_dr_l[t, 0] = cumr/cumw","    t_dr_l[t, 0] = cumr","  return t_dr_l","","# #######################################  Value Estimator  ###################################### #","class VEster(object): # Value Estimator","  def __init__(self, s_len, nn_len):","    self.s_len = s_len","    self.nn_len = nn_len","    ","    self.init()","  ","  def __repr__(self):","    return \"VEster[s_len= {}]\".format(self.s_len)","  ","  def init(self):","    # N x T x s_len","    self.s_ph = tf.placeholder(shape=(None, None, self.s_len), dtype=tf.float32)","    # self.hidden1 = tf.contrib.layers.fully_connected(self.s_ph, self.nn_len, activation_fn=tf.nn.relu)","    # self.hidden2 = tf.contrib.layers.fully_connected(self.hidden1, self.nn_len, activation_fn=tf.nn.relu)","    # self.v = tf.contrib.layers.fully_connected(self.hidden2, 1, activation_fn=None)","    self.hidden = tf.contrib.layers.fully_connected(self.s_ph, self.nn_len, activation_fn=tf.nn.relu, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    self.v = tf.contrib.layers.fully_connected(self.hidden, 1, activation_fn=None, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    ","    self.sampled_v = tf.placeholder(shape=(None, None, 1), dtype=tf.float32)","    # self.loss = tf.reduce_sum(tf.squared_difference(self.v, self.sampled_v) )","    self.loss = tf.losses.mean_squared_error(self.v, self.sampled_v) + \\","      tf.losses.get_regularization_loss()","    ","    # self.optimizer = tf.train.GradientDescentOptimizer(0.01)","    self.optimizer = tf.train.AdamOptimizer(0.01)","    self.train_op = self.optimizer.minimize(self.loss)","    ","    self.sess = tf.Session()","    self.sess.run(tf.global_variables_initializer() )","  ","  def train_w_mult_trajs(self, n_t_s_l, n_t_v_l):","    _, loss = self.sess.run([self.train_op, self.loss],","                            feed_dict={self.s_ph: n_t_s_l,","                                       self.sampled_v: n_t_v_l} )","    print(\"VEster:: loss= {}\".format(loss) )","  ","  def get_v(self, n_t_s_l):","    return self.sess.run(self.v,","                         feed_dict={self.s_ph: n_t_s_l} )","","# #############################################  Learner  ###################################### #","class Learner(object):","  def __init__(self, s_len, a_len, nn_len):","    self.s_len = s_len","    self.a_len = a_len","    self.nn_len = nn_len","    ","    self.gamma = 0.99 # 0.8","    ","    self.saver = None","    self.sess = None","  ","  def save(self, step):","    save_name = 'save/{}'.format(self)","    save_path = self.saver.save(self.sess, save_name, global_step=step)","    log(WARNING, \"saved; \", save_path=save_path)","  ","  def restore(self, step):","    save_name = 'save/{}-{}'.format(self, step)","    try:","      save_path = self.saver.restore(self.sess, save_name)","      # log(WARNING, \"restored; \", save_path=save_path)","      return True","    except:","      return False","","# ####################################  Policy Gradient Learner  ################################# #","class PolicyGradLearner(Learner):","  def __init__(self, s_len, a_len, nn_len=10, w_actorcritic=False):","    super().__init__(s_len, a_len, nn_len)","    self.w_actorcritic = w_actorcritic","    ","    self.v_ester = VEster(s_len, nn_len)","    self.init()","    self.saver = tf.train.Saver(max_to_keep=5)","    ","    # self.save_name = 'save/PolicyGradLearner_gamma{}_slen{}_alen{}_nnlen{}_wactorcritic{}'.format(self.gamma, s_len, a_len, nn_len, w_actorcritic)","  ","  def __repr__(self):","    return 'PolicyGradLearner(s_len= {}, a_len= {}, nn_len= {}, gamma= {}, w_actorcritic= {})'.format(self.s_len, self.a_len, self.nn_len, self.gamma, self.w_actorcritic)","  ","  def init(self):","    # N x T x s_len","    self.s_ph = tf.placeholder(tf.float32, shape=(None, None, self.s_len) )","    hidden1 = tf.contrib.layers.fully_connected(self.s_ph, self.nn_len, activation_fn=tf.nn.relu, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    hidden2 = tf.contrib.layers.fully_connected(hidden1, self.nn_len, activation_fn=tf.nn.relu, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    self.a_probs = tf.contrib.layers.fully_connected(hidden2, self.a_len, activation_fn=tf.nn.softmax, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    # self.a_probs = tf.contrib.layers.fully_connected(hidden1, self.a_len, activation_fn=tf.nn.softmax, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    ","    self.a_ph = tf.placeholder(tf.int32, shape=(None, None, 1), name='a_ph')","    self.q_ph = tf.placeholder(tf.float32, shape=(None, None, 1), name='q_ph')","    self.v_ph = tf.placeholder(tf.float32, shape=(None, None, 1), name='v_ph')","    ","    sh = tf.shape(self.a_probs)","    N, T = sh[0], sh[1]","    indices = tf.range(0, N*T)*sh[2] + tf.reshape(self.a_ph, [-1] )","    self.resp_outputs = tf.reshape(tf.gather(tf.reshape(self.a_probs, [-1] ), indices), (N, T, 1) )","    self.loss = \\","      -tf.reduce_mean(tf.reduce_sum(tf.log(self.resp_outputs)*(self.q_ph - self.v_ph), axis=1), axis=0) + \\","      tf.losses.get_regularization_loss()","    ","    self.optimizer = tf.train.AdamOptimizer(0.01) # tf.train.GradientDescentOptimizer(0.01)","    self.train_op = self.optimizer.minimize(self.loss)","    ","    self.sess = tf.Session()","    self.sess.run(tf.global_variables_initializer() )","  ","  def train_w_mult_trajs(self, n_t_s_l, n_t_a_l, n_t_r_l):","    # All trajectories use the same policy","    N = len(n_t_s_l)","    T = len(n_t_s_l[0] )","    # print(\"n_t_s_l.shape= {}\".format(n_t_s_l.shape) )","    # print(\"avg r= {}\".format(np.mean(n_t_r_l) ) )","    ","    if not self.w_actorcritic:","      n_t_q_l = np.zeros((N, T, 1))","      for n in range(N):","        n_t_q_l[n] = rewards_to_qvals(n_t_r_l[n], self.gamma)","      # print(\"n_t_q_l= {}\".format(n_t_q_l) )","      # print(\"n_t_q_l.shape= {}\".format(n_t_q_l.shape) )","      print(\"PolicyGradLearner:: avg q= {}\".format(np.mean(n_t_q_l) ) )","      ","      t_avgq_l = np.array([np.mean(n_t_q_l[:, t, 0] ) for t in range(T) ] ).reshape((T, 1))","      # m = np.mean(n_t_q_l)","      # t_avgq_l = np.array([m for t in range(T) ] ).reshape((T, 1))","      n_t_v_l = np.zeros((N, T, 1))","      for n in range(N):","        n_t_v_l[n] = t_avgq_l","      # print(\"n_t_v_l= {}\".format(n_t_v_l) )","      # print(\"n_t_v_l.shape= {}\".format(n_t_v_l.shape) )","      ","      loss, _ = self.sess.run([self.loss, self.train_op],","                              feed_dict={self.s_ph: n_t_s_l,","                                         self.a_ph: n_t_a_l,","                                         self.q_ph: n_t_q_l,","                                         self.v_ph: n_t_v_l} )","    else:","      # Policy gradient by getting baseline values from actor-critic","      n_t_v_l = np.zeros((N, T, 1))","      n_t_vest_l = self.v_ester.get_v(n_t_s_l)","      for t in range(T-1):","        n_t_v_l[:, t] = n_t_r_l[:, t] + self.gamma*n_t_vest_l[:, t+1]","      n_t_v_l[:, T-1] = n_t_r_l[:, T-1]","      self.v_ester.train_w_mult_trajs(n_t_s_l, n_t_v_l)","      ","      n_t_v_l = self.v_ester.get_v(n_t_s_l)","      n_t_q_l = np.zeros((N, T, 1))","      # for n in range(N):","      #   for t in range(T-1):","      #     n_t_q_l[n, t] = n_t_r_l[n, t] + self.gamma*n_t_v_l[n, t+1]","      #   n_t_q_l[n, T-1] = n_t_r_l[n, t]","      for t in range(T-1):","        n_t_q_l[:, t] = n_t_r_l[:, t] + self.gamma*n_t_v_l[:, t+1]","      n_t_q_l[:, T-1] = n_t_r_l[:, T-1]","      loss, _ = self.sess.run([self.loss, self.train_op],","                              feed_dict={self.s_ph: n_t_s_l,","                                         self.a_ph: n_t_a_l,","                                         self.q_ph: n_t_q_l,","                                         self.v_ph: n_t_v_l} )","    log(INFO, \"PolicyGradLearner;\", loss=loss)","  ","  def get_action_dist(self, s):","    a_probs = self.sess.run(self.a_probs, feed_dict={self.s_ph: [[s]] } )","    return np.array(a_probs[0][0] )","  ","  def get_random_action(self, s):","    a_probs = self.sess.run(self.a_probs, feed_dict={self.s_ph: [[s]] } )","    a_dist = np.array(a_probs[0][0] )","    # log(WARNING, \"\", s=s, a_dist=a_dist)","    a = np.random.choice(a_dist, 1, p=a_dist)","    a = np.argmax(a_dist == a)","    return a","  ","  def get_max_action(self, s):","    a_probs = self.sess.run(self.a_probs, feed_dict={self.s_ph: [[s]] } )","    a_dist = a_probs[0][0]","    # print(\"a_dist= {}\".format(a_dist) )","    return np.argmax(a_dist)  ","","# ###########################################  Q Learning  ####################################### #","class QLearner(Learner):","  def __init__(self, s_len, a_len, nn_len=10):","    super().__init__(s_len, a_len, nn_len)","    self.eps = 0.1","    self.init()","    self.saver = tf.train.Saver(max_to_keep=5)","    ","  def __repr__(self):","    return 'QLearner(s_len= {}, a_len= {})'.format(self.s_len, self.a_len)","  ","  def init(self):","    # N x T x s_len","    self.s_ph = tf.placeholder(tf.float32, shape=(None, None, self.s_len) )","    hidden1 = tf.contrib.layers.fully_connected(self.s_ph, self.nn_len, activation_fn=tf.nn.relu)","    hidden2 = tf.contrib.layers.fully_connected(hidden1, self.nn_len, activation_fn=tf.nn.relu)","    self.Qa_ph = tf.contrib.layers.fully_connected(hidden2, self.a_len, activation_fn=None)","    ","    self.a_ph = tf.placeholder(tf.int32, shape=(None, None, 1), name=\"a_ph\")","    self.targetq_ph = tf.placeholder(tf.float32, shape=(None, None, 1), name=\"q_ph\")","    ","    sh = tf.shape(self.Qa_ph)","    N, T = sh[0], sh[1]","    indices = tf.range(0, N*T)*sh[2] + tf.reshape(self.a_ph, [-1] )","    self.resp_outputs = tf.reshape(tf.gather(tf.reshape(self.Qa_ph, [-1] ), indices), (sh[0], sh[1], 1) )","    self.loss = tf.losses.mean_squared_error(self.resp_outputs, self.targetq_ph)","    ","    self.optimizer = tf.train.AdamOptimizer(0.01)","    self.train_op = self.optimizer.minimize(self.loss)","    ","    self.sess = tf.Session()","    self.sess.run(tf.global_variables_initializer() )","  ","  def train_w_mult_trajs(self, n_t_s_l, n_t_a_l, n_t_r_l):","    N = len(n_t_s_l)","    T = len(n_t_s_l[0] )","    ","    n_t_q_l = self.sess.run(self.Qa_ph,","                            feed_dict={self.s_ph: n_t_s_l} )","    n_t_targetq_l = np.zeros((N, T, 1))","    for n in range(N):","      for t in range(T):","        if t < T-1:","          n_t_targetq_l[n, t, 0] = n_t_r_l[n, t, 0] + self.gamma*max(n_t_q_l[n, t+1, :] )","        else:","          n_t_targetq_l[n, t, 0] = n_t_r_l[n, t, 0]","    ","    # n_t_targetq_l = np.zeros((N, T, 1))","    # for n in range(N):","    #   n_t_targetq_l[n] = rewards_to_qvals(n_t_r_l[n], self.gamma)","    ","    loss, _ = self.sess.run([self.loss, self.train_op],","                            feed_dict={self.s_ph: n_t_s_l,","                                       self.a_ph: n_t_a_l,","                                       self.targetq_ph: n_t_targetq_l} )","    print(\"QLearner:: loss= {}\".format(loss) )","    # self.eps *= 0.95","  ","  def get_random_action(self, s):","    if random.uniform(0, 1) < self.eps:","      return np.random.randint(self.a_len, size=1)[0]","    else:","      qa_l = self.sess.run(self.Qa_ph,","                           feed_dict={self.s_ph: [[s]] } )","      return np.argmax(qa_l)","  ","  def get_max_action(self, s):","    qa_l = self.sess.run(self.Qa_ph,","                         feed_dict={self.s_ph: [[s]] } )","    return np.argmax(qa_l)","","# #############################################  Test  ########################################### #","def test():","  s_len, a_len, nn_len = 3, 3, 10","  scher = PolicyGradLearner(s_len, a_len, nn_len)","  # scher = QLearner(s_len, a_len, nn_len)","  ","  def state():","    s = np.random.randint(10, size=s_len)","    sum_s = sum(s)","    return s/sum_s if sum_s != 0 else s","  ","  def reward(s, a):","    # s_min = min(s)","    # r = 10 if s[a] == s_min else 0","    # return min(100, 1/(0.001 + s[a] - min(s) ) )","    # return 100*math.exp(-(s[a] - min(s) ) )","    return 1/(0.1 + s[a] - min(s) )","  ","  def evaluate():","    num_shortest_found = 0","    for e in range(100):","      s = state()","      a = scher.get_max_action(s)","      if s[a] - min(s) < 0.01:","        num_shortest_found += 1","    print(\"freq shortest found= {}\".format(num_shortest_found/100) )","  ","  def train_w_mult_trajs():","    N, T = 10, 100","    def gen_N_traj():","      n_t_s_l, n_t_a_l, n_t_r_l = np.zeros((N, T, s_len)), np.zeros((N, T, 1)), np.zeros((N, T, 1))","      for n in range(N):","        for t in range(T):","          s = state()","          a = scher.get_random_action(s)","          n_t_s_l[n, t, :] = s","          n_t_a_l[n, t, :] = a","          n_t_r_l[n, t, :] = reward(s, a)","      return n_t_s_l, n_t_a_l, n_t_r_l","    ","    for i in range(100*20):","      n_t_s_l, n_t_a_l, n_t_r_l = gen_N_traj()","      scher.train_w_mult_trajs(n_t_s_l, n_t_a_l, n_t_r_l)","      if i % 10 == 0:","        evaluate()","  train_w_mult_trajs()","","def vsimple_regress():","  s_len = 3","  T = 100","  def state():","    s = np.random.randint(10, size=s_len)","    sum_s = sum(s)","    return s/sum_s if sum_s != 0 else s","  ","  def reward(s):","    return 10*max(s)","  ","  def sample_traj():","    t_s_l, t_r_l = np.zeros((T, s_len)), np.zeros((T, 1))","    for t in range(T):","      s = state()","      t_s_l[t, :] = s","      t_r_l[t, :] = reward(s)","    return t_s_l, t_r_l","  ","  value_ester = VEster(s_len, nn_len=10, straj_training=False)","  for i in range(100*40):","    t_s_l, t_r_l = sample_traj()","    value_ester.train_w_single_traj(t_s_l, t_r_l)","","class A(object):","  def __init__(self):","    self.a = 'A'","  ","  def __repr__(self):","    return self.a","  ","  def test(self):","    print(\"self.a= {}\".format(self.a) )","  ","class B(A):","  def __init__(self):","    super().__init__()","    ","    self.init()","  ","  def init(self):","    self.a = 'B'","","if __name__ == \"__main__\":","  # test()","  # vsimple_regress()","  ","  # b = B()","  # # print(\"b= {}\".format(b) )","  # b.test()","  ","  # learner = PolicyGradLearner(s_len=1, a_len=1)","  learner = QLearner(s_len=1, a_len=1)","  learner.save(0)","  restore_result = learner.restore(0)","  print(\"restore_result= {}\".format(restore_result) )"]},{"action":"insertText","range":{"start":{"row":0,"column":0},"end":{"row":0,"column":25}},"text":"import math, time, random"},{"action":"insertText","range":{"start":{"row":0,"column":25},"end":{"row":1,"column":0}},"text":"\n"},{"action":"insertLines","range":{"start":{"row":1,"column":0},"end":{"row":384,"column":0}},"lines":["import numpy as np","import tensorflow as tf","","from log_utils import *","","def rewards_to_qvals(t_r_l, gamma):","  T = t_r_l.shape[0]","  # reward = average of all following rewards","  # for t in range(T):","  #   t_r_l[t, 0] = np.mean(t_r_l[t:, 0])","  ","  # for t in range(T):","  #   cumw, cumr = 0, 0","  #   for i, r in enumerate(t_r_l[t:, 0] ):","  #     cumw += gamma**i","  #     cumr += gamma**i * r","  #   t_r_l[t, 0] = cumr/cumw","  # return t_r_l","  ","  t_dr_l = np.zeros((T, 1))","  cumw, cumr = 0, 0","  for t in range(T-1, -1, -1):","    cumr = t_r_l[t, 0] + gamma*cumr","    # cumw = 1 + gamma*cumw","    # t_dr_l[t, 0] = cumr/cumw","    t_dr_l[t, 0] = cumr","  return t_dr_l","","# #######################################  Value Estimator  ###################################### #","class VEster(object): # Value Estimator","  def __init__(self, s_len, nn_len):","    self.s_len = s_len","    self.nn_len = nn_len","    ","    self.init()","  ","  def __repr__(self):","    return \"VEster[s_len= {}]\".format(self.s_len)","  ","  def init(self):","    # N x T x s_len","    self.s_ph = tf.placeholder(shape=(None, None, self.s_len), dtype=tf.float32)","    # self.hidden1 = tf.contrib.layers.fully_connected(self.s_ph, self.nn_len, activation_fn=tf.nn.relu)","    # self.hidden2 = tf.contrib.layers.fully_connected(self.hidden1, self.nn_len, activation_fn=tf.nn.relu)","    # self.v = tf.contrib.layers.fully_connected(self.hidden2, 1, activation_fn=None)","    self.hidden = tf.contrib.layers.fully_connected(self.s_ph, self.nn_len, activation_fn=tf.nn.relu, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    self.v = tf.contrib.layers.fully_connected(self.hidden, 1, activation_fn=None, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    ","    self.sampled_v = tf.placeholder(shape=(None, None, 1), dtype=tf.float32)","    # self.loss = tf.reduce_sum(tf.squared_difference(self.v, self.sampled_v) )","    self.loss = tf.losses.mean_squared_error(self.v, self.sampled_v) + \\","      tf.losses.get_regularization_loss()","    ","    # self.optimizer = tf.train.GradientDescentOptimizer(0.01)","    self.optimizer = tf.train.AdamOptimizer(0.01)","    self.train_op = self.optimizer.minimize(self.loss)","    ","    self.sess = tf.Session()","    self.sess.run(tf.global_variables_initializer() )","  ","  def train_w_mult_trajs(self, n_t_s_l, n_t_v_l):","    _, loss = self.sess.run([self.train_op, self.loss],","                            feed_dict={self.s_ph: n_t_s_l,","                                       self.sampled_v: n_t_v_l} )","    print(\"VEster:: loss= {}\".format(loss) )","  ","  def get_v(self, n_t_s_l):","    return self.sess.run(self.v,","                         feed_dict={self.s_ph: n_t_s_l} )","","# #############################################  Learner  ###################################### #","class Learner(object):","  def __init__(self, s_len, a_len, nn_len):","    self.s_len = s_len","    self.a_len = a_len","    self.nn_len = nn_len","    ","    self.gamma = 0.99 # 0.8","    ","    self.saver = None","    self.sess = None","  ","  def save(self, step):","    save_name = 'save/{}'.format(self)","    save_path = self.saver.save(self.sess, save_name, global_step=step)","    log(WARNING, \"saved; \", save_path=save_path)","  ","  def restore(self, step):","    save_name = 'save/{}-{}'.format(self, step)","    try:","      save_path = self.saver.restore(self.sess, save_name)","      # log(WARNING, \"restored; \", save_path=save_path)","      return True","    except:","      return False","","# ####################################  Policy Gradient Learner  ################################# #","class PolicyGradLearner(Learner):","  def __init__(self, s_len, a_len, nn_len=10, w_actorcritic=False):","    super().__init__(s_len, a_len, nn_len)","    self.w_actorcritic = w_actorcritic","    ","    self.v_ester = VEster(s_len, nn_len)","    self.init()","    self.saver = tf.train.Saver(max_to_keep=5)","    ","    # self.save_name = 'save/PolicyGradLearner_gamma{}_slen{}_alen{}_nnlen{}_wactorcritic{}'.format(self.gamma, s_len, a_len, nn_len, w_actorcritic)","  ","  def __repr__(self):","    return 'PolicyGradLearner(s_len= {}, a_len= {}, nn_len= {}, gamma= {}, w_actorcritic= {})'.format(self.s_len, self.a_len, self.nn_len, self.gamma, self.w_actorcritic)","  ","  def init(self):","    # N x T x s_len","    self.s_ph = tf.placeholder(tf.float32, shape=(None, None, self.s_len) )","    hidden1 = tf.contrib.layers.fully_connected(self.s_ph, self.nn_len, activation_fn=tf.nn.relu, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    hidden2 = tf.contrib.layers.fully_connected(hidden1, self.nn_len, activation_fn=tf.nn.relu, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    self.a_probs = tf.contrib.layers.fully_connected(hidden2, self.a_len, activation_fn=tf.nn.softmax, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    # self.a_probs = tf.contrib.layers.fully_connected(hidden1, self.a_len, activation_fn=tf.nn.softmax, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    ","    self.a_ph = tf.placeholder(tf.int32, shape=(None, None, 1), name='a_ph')","    self.q_ph = tf.placeholder(tf.float32, shape=(None, None, 1), name='q_ph')","    self.v_ph = tf.placeholder(tf.float32, shape=(None, None, 1), name='v_ph')","    ","    sh = tf.shape(self.a_probs)","    N, T = sh[0], sh[1]","    indices = tf.range(0, N*T)*sh[2] + tf.reshape(self.a_ph, [-1] )","    self.resp_outputs = tf.reshape(tf.gather(tf.reshape(self.a_probs, [-1] ), indices), (N, T, 1) )","    self.loss = \\","      -tf.reduce_mean(tf.reduce_sum(tf.log(self.resp_outputs)*(self.q_ph - self.v_ph), axis=1), axis=0) + \\","      tf.losses.get_regularization_loss()","    ","    self.optimizer = tf.train.AdamOptimizer(0.01) # tf.train.GradientDescentOptimizer(0.01)","    self.train_op = self.optimizer.minimize(self.loss)","    ","    self.sess = tf.Session()","    self.sess.run(tf.global_variables_initializer() )","  ","  def train_w_mult_trajs(self, n_t_s_l, n_t_a_l, n_t_r_l):","    # All trajectories use the same policy","    N = len(n_t_s_l)","    T = len(n_t_s_l[0] )","    # print(\"n_t_s_l.shape= {}\".format(n_t_s_l.shape) )","    # print(\"avg r= {}\".format(np.mean(n_t_r_l) ) )","    ","    if not self.w_actorcritic:","      n_t_q_l = np.zeros((N, T, 1))","      for n in range(N):","        n_t_q_l[n] = rewards_to_qvals(n_t_r_l[n], self.gamma)","      # print(\"n_t_q_l= {}\".format(n_t_q_l) )","      # print(\"n_t_q_l.shape= {}\".format(n_t_q_l.shape) )","      print(\"PolicyGradLearner:: avg q= {}\".format(np.mean(n_t_q_l) ) )","      ","      t_avgq_l = np.array([np.mean(n_t_q_l[:, t, 0] ) for t in range(T) ] ).reshape((T, 1))","      # m = np.mean(n_t_q_l)","      # t_avgq_l = np.array([m for t in range(T) ] ).reshape((T, 1))","      n_t_v_l = np.zeros((N, T, 1))","      for n in range(N):","        n_t_v_l[n] = t_avgq_l","      # print(\"n_t_v_l= {}\".format(n_t_v_l) )","      # print(\"n_t_v_l.shape= {}\".format(n_t_v_l.shape) )","      ","      loss, _ = self.sess.run([self.loss, self.train_op],","                              feed_dict={self.s_ph: n_t_s_l,","                                         self.a_ph: n_t_a_l,","                                         self.q_ph: n_t_q_l,","                                         self.v_ph: n_t_v_l} )","    else:","      # Policy gradient by getting baseline values from actor-critic","      n_t_v_l = np.zeros((N, T, 1))","      n_t_vest_l = self.v_ester.get_v(n_t_s_l)","      for t in range(T-1):","        n_t_v_l[:, t] = n_t_r_l[:, t] + self.gamma*n_t_vest_l[:, t+1]","      n_t_v_l[:, T-1] = n_t_r_l[:, T-1]","      self.v_ester.train_w_mult_trajs(n_t_s_l, n_t_v_l)","      ","      n_t_v_l = self.v_ester.get_v(n_t_s_l)","      n_t_q_l = np.zeros((N, T, 1))","      # for n in range(N):","      #   for t in range(T-1):","      #     n_t_q_l[n, t] = n_t_r_l[n, t] + self.gamma*n_t_v_l[n, t+1]","      #   n_t_q_l[n, T-1] = n_t_r_l[n, t]","      for t in range(T-1):","        n_t_q_l[:, t] = n_t_r_l[:, t] + self.gamma*n_t_v_l[:, t+1]","      n_t_q_l[:, T-1] = n_t_r_l[:, T-1]","      loss, _ = self.sess.run([self.loss, self.train_op],","                              feed_dict={self.s_ph: n_t_s_l,","                                         self.a_ph: n_t_a_l,","                                         self.q_ph: n_t_q_l,","                                         self.v_ph: n_t_v_l} )","    log(INFO, \"PolicyGradLearner;\", loss=loss)","  ","  def get_action_dist(self, s):","    a_probs = self.sess.run(self.a_probs, feed_dict={self.s_ph: [[s]] } )","    return np.array(a_probs[0][0] )","  ","  def get_random_action(self, s):","    a_probs = self.sess.run(self.a_probs, feed_dict={self.s_ph: [[s]] } )","    a_dist = np.array(a_probs[0][0] )","    # log(WARNING, \"\", s=s, a_dist=a_dist)","    a = np.random.choice(a_dist, 1, p=a_dist)","    a = np.argmax(a_dist == a)","    return a","  ","  def get_max_action(self, s):","    a_probs = self.sess.run(self.a_probs, feed_dict={self.s_ph: [[s]] } )","    a_dist = a_probs[0][0]","    # print(\"a_dist= {}\".format(a_dist) )","    return np.argmax(a_dist)  ","","# ###########################################  Q Learning  ####################################### #","class QLearner(Learner):","  def __init__(self, s_len, a_len, nn_len=10):","    super().__init__(s_len, a_len, nn_len)","    self.eps = 0.1","    self.init()","    self.saver = tf.train.Saver(max_to_keep=5)","    ","  def __repr__(self):","    return 'QLearner(s_len= {}, a_len= {})'.format(self.s_len, self.a_len)","  ","  def init(self):","    # N x T x s_len","    self.s_ph = tf.placeholder(tf.float32, shape=(None, None, self.s_len) )","    hidden1 = tf.contrib.layers.fully_connected(self.s_ph, self.nn_len, activation_fn=tf.nn.relu)","    hidden2 = tf.contrib.layers.fully_connected(hidden1, self.nn_len, activation_fn=tf.nn.relu)","    self.Qa_ph = tf.contrib.layers.fully_connected(hidden2, self.a_len, activation_fn=None)","    ","    self.a_ph = tf.placeholder(tf.int32, shape=(None, None, 1), name=\"a_ph\")","    self.targetq_ph = tf.placeholder(tf.float32, shape=(None, None, 1), name=\"q_ph\")","    ","    sh = tf.shape(self.Qa_ph)","    N, T = sh[0], sh[1]","    indices = tf.range(0, N*T)*sh[2] + tf.reshape(self.a_ph, [-1] )","    self.resp_outputs = tf.reshape(tf.gather(tf.reshape(self.Qa_ph, [-1] ), indices), (N, T, 1) )","    self.loss = tf.losses.mean_squared_error(self.resp_outputs, self.targetq_ph)","    ","    self.optimizer = tf.train.AdamOptimizer(0.01)","    self.train_op = self.optimizer.minimize(self.loss)","    ","    self.sess = tf.Session()","    self.sess.run(tf.global_variables_initializer() )","  ","  def train_w_mult_trajs(self, n_t_s_l, n_t_a_l, n_t_r_l):","    N = len(n_t_s_l)","    T = len(n_t_s_l[0] )","    ","    n_t_q_l = self.sess.run(self.Qa_ph,","                            feed_dict={self.s_ph: n_t_s_l} )","    n_t_targetq_l = np.zeros((N, T, 1))","    for n in range(N):","      for t in range(T):","        if t < T-1:","          n_t_targetq_l[n, t, 0] = n_t_r_l[n, t, 0] + self.gamma*max(n_t_q_l[n, t+1, :] )","        else:","          n_t_targetq_l[n, t, 0] = n_t_r_l[n, t, 0]","    ","    # n_t_targetq_l = np.zeros((N, T, 1))","    # for n in range(N):","    #   n_t_targetq_l[n] = rewards_to_qvals(n_t_r_l[n], self.gamma)","    ","    loss, _ = self.sess.run([self.loss, self.train_op],","                            feed_dict={self.s_ph: n_t_s_l,","                                       self.a_ph: n_t_a_l,","                                       self.targetq_ph: n_t_targetq_l} )","    print(\"QLearner:: loss= {}\".format(loss) )","    # self.eps *= 0.95","  ","  def get_random_action(self, s):","    if random.uniform(0, 1) < self.eps:","      return np.random.randint(self.a_len, size=1)[0]","    else:","      qa_l = self.sess.run(self.Qa_ph,","                           feed_dict={self.s_ph: [[s]] } )","      return np.argmax(qa_l)","  ","  def get_max_action(self, s):","    qa_l = self.sess.run(self.Qa_ph,","                         feed_dict={self.s_ph: [[s]] } )","    return np.argmax(qa_l)","","# #############################################  Test  ########################################### #","def test():","  s_len, a_len, nn_len = 3, 3, 10","  scher = PolicyGradLearner(s_len, a_len, nn_len)","  # scher = QLearner(s_len, a_len, nn_len)","  ","  def state():","    s = np.random.randint(10, size=s_len)","    sum_s = sum(s)","    return s/sum_s if sum_s != 0 else s","  ","  def reward(s, a):","    # s_min = min(s)","    # r = 10 if s[a] == s_min else 0","    # return min(100, 1/(0.001 + s[a] - min(s) ) )","    # return 100*math.exp(-(s[a] - min(s) ) )","    return 1/(0.1 + s[a] - min(s) )","  ","  def evaluate():","    num_shortest_found = 0","    for e in range(100):","      s = state()","      a = scher.get_max_action(s)","      if s[a] - min(s) < 0.01:","        num_shortest_found += 1","    print(\"freq shortest found= {}\".format(num_shortest_found/100) )","  ","  def train_w_mult_trajs():","    N, T = 10, 100","    def gen_N_traj():","      n_t_s_l, n_t_a_l, n_t_r_l = np.zeros((N, T, s_len)), np.zeros((N, T, 1)), np.zeros((N, T, 1))","      for n in range(N):","        for t in range(T):","          s = state()","          a = scher.get_random_action(s)","          n_t_s_l[n, t, :] = s","          n_t_a_l[n, t, :] = a","          n_t_r_l[n, t, :] = reward(s, a)","      return n_t_s_l, n_t_a_l, n_t_r_l","    ","    for i in range(100*20):","      n_t_s_l, n_t_a_l, n_t_r_l = gen_N_traj()","      scher.train_w_mult_trajs(n_t_s_l, n_t_a_l, n_t_r_l)","      if i % 10 == 0:","        evaluate()","  train_w_mult_trajs()","","def vsimple_regress():","  s_len = 3","  T = 100","  def state():","    s = np.random.randint(10, size=s_len)","    sum_s = sum(s)","    return s/sum_s if sum_s != 0 else s","  ","  def reward(s):","    return 10*max(s)","  ","  def sample_traj():","    t_s_l, t_r_l = np.zeros((T, s_len)), np.zeros((T, 1))","    for t in range(T):","      s = state()","      t_s_l[t, :] = s","      t_r_l[t, :] = reward(s)","    return t_s_l, t_r_l","  ","  value_ester = VEster(s_len, nn_len=10, straj_training=False)","  for i in range(100*40):","    t_s_l, t_r_l = sample_traj()","    value_ester.train_w_single_traj(t_s_l, t_r_l)","","class A(object):","  def __init__(self):","    self.a = 'A'","  ","  def __repr__(self):","    return self.a","  ","  def test(self):","    print(\"self.a= {}\".format(self.a) )","  ","class B(A):","  def __init__(self):","    super().__init__()","    ","    self.init()","  ","  def init(self):","    self.a = 'B'","","if __name__ == \"__main__\":","  # test()","  # vsimple_regress()","  ","  # b = B()","  # # print(\"b= {}\".format(b) )","  # b.test()","  ","  # learner = PolicyGradLearner(s_len=1, a_len=1)","  learner = QLearner(s_len=1, a_len=1)","  learner.save(0)","  restore_result = learner.restore(0)","  print(\"restore_result= {}\".format(restore_result) )"]}]}],[{"group":"doc","deltas":[{"action":"removeLines","range":{"start":{"row":0,"column":0},"end":{"row":384,"column":0}},"nl":"\n","lines":["import math, time, random","import numpy as np","import tensorflow as tf","","from log_utils import *","","def rewards_to_qvals(t_r_l, gamma):","  T = t_r_l.shape[0]","  # reward = average of all following rewards","  # for t in range(T):","  #   t_r_l[t, 0] = np.mean(t_r_l[t:, 0])","  ","  # for t in range(T):","  #   cumw, cumr = 0, 0","  #   for i, r in enumerate(t_r_l[t:, 0] ):","  #     cumw += gamma**i","  #     cumr += gamma**i * r","  #   t_r_l[t, 0] = cumr/cumw","  # return t_r_l","  ","  t_dr_l = np.zeros((T, 1))","  cumw, cumr = 0, 0","  for t in range(T-1, -1, -1):","    cumr = t_r_l[t, 0] + gamma*cumr","    # cumw = 1 + gamma*cumw","    # t_dr_l[t, 0] = cumr/cumw","    t_dr_l[t, 0] = cumr","  return t_dr_l","","# #######################################  Value Estimator  ###################################### #","class VEster(object): # Value Estimator","  def __init__(self, s_len, nn_len):","    self.s_len = s_len","    self.nn_len = nn_len","    ","    self.init()","  ","  def __repr__(self):","    return \"VEster[s_len= {}]\".format(self.s_len)","  ","  def init(self):","    # N x T x s_len","    self.s_ph = tf.placeholder(shape=(None, None, self.s_len), dtype=tf.float32)","    # self.hidden1 = tf.contrib.layers.fully_connected(self.s_ph, self.nn_len, activation_fn=tf.nn.relu)","    # self.hidden2 = tf.contrib.layers.fully_connected(self.hidden1, self.nn_len, activation_fn=tf.nn.relu)","    # self.v = tf.contrib.layers.fully_connected(self.hidden2, 1, activation_fn=None)","    self.hidden = tf.contrib.layers.fully_connected(self.s_ph, self.nn_len, activation_fn=tf.nn.relu, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    self.v = tf.contrib.layers.fully_connected(self.hidden, 1, activation_fn=None, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    ","    self.sampled_v = tf.placeholder(shape=(None, None, 1), dtype=tf.float32)","    # self.loss = tf.reduce_sum(tf.squared_difference(self.v, self.sampled_v) )","    self.loss = tf.losses.mean_squared_error(self.v, self.sampled_v) + \\","      tf.losses.get_regularization_loss()","    ","    # self.optimizer = tf.train.GradientDescentOptimizer(0.01)","    self.optimizer = tf.train.AdamOptimizer(0.01)","    self.train_op = self.optimizer.minimize(self.loss)","    ","    self.sess = tf.Session()","    self.sess.run(tf.global_variables_initializer() )","  ","  def train_w_mult_trajs(self, n_t_s_l, n_t_v_l):","    _, loss = self.sess.run([self.train_op, self.loss],","                            feed_dict={self.s_ph: n_t_s_l,","                                       self.sampled_v: n_t_v_l} )","    print(\"VEster:: loss= {}\".format(loss) )","  ","  def get_v(self, n_t_s_l):","    return self.sess.run(self.v,","                         feed_dict={self.s_ph: n_t_s_l} )","","# #############################################  Learner  ###################################### #","class Learner(object):","  def __init__(self, s_len, a_len, nn_len):","    self.s_len = s_len","    self.a_len = a_len","    self.nn_len = nn_len","    ","    self.gamma = 0.99 # 0.8","    ","    self.saver = None","    self.sess = None","  ","  def save(self, step):","    save_name = 'save/{}'.format(self)","    save_path = self.saver.save(self.sess, save_name, global_step=step)","    log(WARNING, \"saved; \", save_path=save_path)","  ","  def restore(self, step):","    save_name = 'save/{}-{}'.format(self, step)","    try:","      save_path = self.saver.restore(self.sess, save_name)","      # log(WARNING, \"restored; \", save_path=save_path)","      return True","    except:","      return False","","# ####################################  Policy Gradient Learner  ################################# #","class PolicyGradLearner(Learner):","  def __init__(self, s_len, a_len, nn_len=10, w_actorcritic=False):","    super().__init__(s_len, a_len, nn_len)","    self.w_actorcritic = w_actorcritic","    ","    self.v_ester = VEster(s_len, nn_len)","    self.init()","    self.saver = tf.train.Saver(max_to_keep=5)","    ","    # self.save_name = 'save/PolicyGradLearner_gamma{}_slen{}_alen{}_nnlen{}_wactorcritic{}'.format(self.gamma, s_len, a_len, nn_len, w_actorcritic)","  ","  def __repr__(self):","    return 'PolicyGradLearner(s_len= {}, a_len= {}, nn_len= {}, gamma= {}, w_actorcritic= {})'.format(self.s_len, self.a_len, self.nn_len, self.gamma, self.w_actorcritic)","  ","  def init(self):","    # N x T x s_len","    self.s_ph = tf.placeholder(tf.float32, shape=(None, None, self.s_len) )","    hidden1 = tf.contrib.layers.fully_connected(self.s_ph, self.nn_len, activation_fn=tf.nn.relu, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    hidden2 = tf.contrib.layers.fully_connected(hidden1, self.nn_len, activation_fn=tf.nn.relu, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    self.a_probs = tf.contrib.layers.fully_connected(hidden2, self.a_len, activation_fn=tf.nn.softmax, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    # self.a_probs = tf.contrib.layers.fully_connected(hidden1, self.a_len, activation_fn=tf.nn.softmax, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    ","    self.a_ph = tf.placeholder(tf.int32, shape=(None, None, 1), name='a_ph')","    self.q_ph = tf.placeholder(tf.float32, shape=(None, None, 1), name='q_ph')","    self.v_ph = tf.placeholder(tf.float32, shape=(None, None, 1), name='v_ph')","    ","    sh = tf.shape(self.a_probs)","    N, T = sh[0], sh[1]","    indices = tf.range(0, N*T)*sh[2] + tf.reshape(self.a_ph, [-1] )","    self.resp_outputs = tf.reshape(tf.gather(tf.reshape(self.a_probs, [-1] ), indices), (N, T, 1) )","    self.loss = \\","      -tf.reduce_mean(tf.reduce_sum(tf.log(self.resp_outputs)*(self.q_ph - self.v_ph), axis=1), axis=0) + \\","      tf.losses.get_regularization_loss()","    ","    self.optimizer = tf.train.AdamOptimizer(0.01) # tf.train.GradientDescentOptimizer(0.01)","    self.train_op = self.optimizer.minimize(self.loss)","    ","    self.sess = tf.Session()","    self.sess.run(tf.global_variables_initializer() )","  ","  def train_w_mult_trajs(self, n_t_s_l, n_t_a_l, n_t_r_l):","    # All trajectories use the same policy","    N = len(n_t_s_l)","    T = len(n_t_s_l[0] )","    # print(\"n_t_s_l.shape= {}\".format(n_t_s_l.shape) )","    # print(\"avg r= {}\".format(np.mean(n_t_r_l) ) )","    ","    if not self.w_actorcritic:","      n_t_q_l = np.zeros((N, T, 1))","      for n in range(N):","        n_t_q_l[n] = rewards_to_qvals(n_t_r_l[n], self.gamma)","      # print(\"n_t_q_l= {}\".format(n_t_q_l) )","      # print(\"n_t_q_l.shape= {}\".format(n_t_q_l.shape) )","      print(\"PolicyGradLearner:: avg q= {}\".format(np.mean(n_t_q_l) ) )","      ","      t_avgq_l = np.array([np.mean(n_t_q_l[:, t, 0] ) for t in range(T) ] ).reshape((T, 1))","      # m = np.mean(n_t_q_l)","      # t_avgq_l = np.array([m for t in range(T) ] ).reshape((T, 1))","      n_t_v_l = np.zeros((N, T, 1))","      for n in range(N):","        n_t_v_l[n] = t_avgq_l","      # print(\"n_t_v_l= {}\".format(n_t_v_l) )","      # print(\"n_t_v_l.shape= {}\".format(n_t_v_l.shape) )","      ","      loss, _ = self.sess.run([self.loss, self.train_op],","                              feed_dict={self.s_ph: n_t_s_l,","                                         self.a_ph: n_t_a_l,","                                         self.q_ph: n_t_q_l,","                                         self.v_ph: n_t_v_l} )","    else:","      # Policy gradient by getting baseline values from actor-critic","      n_t_v_l = np.zeros((N, T, 1))","      n_t_vest_l = self.v_ester.get_v(n_t_s_l)","      for t in range(T-1):","        n_t_v_l[:, t] = n_t_r_l[:, t] + self.gamma*n_t_vest_l[:, t+1]","      n_t_v_l[:, T-1] = n_t_r_l[:, T-1]","      self.v_ester.train_w_mult_trajs(n_t_s_l, n_t_v_l)","      ","      n_t_v_l = self.v_ester.get_v(n_t_s_l)","      n_t_q_l = np.zeros((N, T, 1))","      # for n in range(N):","      #   for t in range(T-1):","      #     n_t_q_l[n, t] = n_t_r_l[n, t] + self.gamma*n_t_v_l[n, t+1]","      #   n_t_q_l[n, T-1] = n_t_r_l[n, t]","      for t in range(T-1):","        n_t_q_l[:, t] = n_t_r_l[:, t] + self.gamma*n_t_v_l[:, t+1]","      n_t_q_l[:, T-1] = n_t_r_l[:, T-1]","      loss, _ = self.sess.run([self.loss, self.train_op],","                              feed_dict={self.s_ph: n_t_s_l,","                                         self.a_ph: n_t_a_l,","                                         self.q_ph: n_t_q_l,","                                         self.v_ph: n_t_v_l} )","    log(INFO, \"PolicyGradLearner;\", loss=loss)","  ","  def get_action_dist(self, s):","    a_probs = self.sess.run(self.a_probs, feed_dict={self.s_ph: [[s]] } )","    return np.array(a_probs[0][0] )","  ","  def get_random_action(self, s):","    a_probs = self.sess.run(self.a_probs, feed_dict={self.s_ph: [[s]] } )","    a_dist = np.array(a_probs[0][0] )","    # log(WARNING, \"\", s=s, a_dist=a_dist)","    a = np.random.choice(a_dist, 1, p=a_dist)","    a = np.argmax(a_dist == a)","    return a","  ","  def get_max_action(self, s):","    a_probs = self.sess.run(self.a_probs, feed_dict={self.s_ph: [[s]] } )","    a_dist = a_probs[0][0]","    # print(\"a_dist= {}\".format(a_dist) )","    return np.argmax(a_dist)  ","","# ###########################################  Q Learning  ####################################### #","class QLearner(Learner):","  def __init__(self, s_len, a_len, nn_len=10):","    super().__init__(s_len, a_len, nn_len)","    self.eps = 0.1","    self.init()","    self.saver = tf.train.Saver(max_to_keep=5)","    ","  def __repr__(self):","    return 'QLearner(s_len= {}, a_len= {})'.format(self.s_len, self.a_len)","  ","  def init(self):","    # N x T x s_len","    self.s_ph = tf.placeholder(tf.float32, shape=(None, None, self.s_len) )","    hidden1 = tf.contrib.layers.fully_connected(self.s_ph, self.nn_len, activation_fn=tf.nn.relu)","    hidden2 = tf.contrib.layers.fully_connected(hidden1, self.nn_len, activation_fn=tf.nn.relu)","    self.Qa_ph = tf.contrib.layers.fully_connected(hidden2, self.a_len, activation_fn=None)","    ","    self.a_ph = tf.placeholder(tf.int32, shape=(None, None, 1), name=\"a_ph\")","    self.targetq_ph = tf.placeholder(tf.float32, shape=(None, None, 1), name=\"q_ph\")","    ","    sh = tf.shape(self.Qa_ph)","    N, T = sh[0], sh[1]","    indices = tf.range(0, N*T)*sh[2] + tf.reshape(self.a_ph, [-1] )","    self.resp_outputs = tf.reshape(tf.gather(tf.reshape(self.Qa_ph, [-1] ), indices), (N, T, 1) )","    self.loss = tf.losses.mean_squared_error(self.resp_outputs, self.targetq_ph)","    ","    self.optimizer = tf.train.AdamOptimizer(0.01)","    self.train_op = self.optimizer.minimize(self.loss)","    ","    self.sess = tf.Session()","    self.sess.run(tf.global_variables_initializer() )","  ","  def train_w_mult_trajs(self, n_t_s_l, n_t_a_l, n_t_r_l):","    N = len(n_t_s_l)","    T = len(n_t_s_l[0] )","    ","    n_t_q_l = self.sess.run(self.Qa_ph,","                            feed_dict={self.s_ph: n_t_s_l} )","    n_t_targetq_l = np.zeros((N, T, 1))","    for n in range(N):","      for t in range(T):","        if t < T-1:","          n_t_targetq_l[n, t, 0] = n_t_r_l[n, t, 0] + self.gamma*max(n_t_q_l[n, t+1, :] )","        else:","          n_t_targetq_l[n, t, 0] = n_t_r_l[n, t, 0]","    ","    # n_t_targetq_l = np.zeros((N, T, 1))","    # for n in range(N):","    #   n_t_targetq_l[n] = rewards_to_qvals(n_t_r_l[n], self.gamma)","    ","    loss, _ = self.sess.run([self.loss, self.train_op],","                            feed_dict={self.s_ph: n_t_s_l,","                                       self.a_ph: n_t_a_l,","                                       self.targetq_ph: n_t_targetq_l} )","    print(\"QLearner:: loss= {}\".format(loss) )","    # self.eps *= 0.95","  ","  def get_random_action(self, s):","    if random.uniform(0, 1) < self.eps:","      return np.random.randint(self.a_len, size=1)[0]","    else:","      qa_l = self.sess.run(self.Qa_ph,","                           feed_dict={self.s_ph: [[s]] } )","      return np.argmax(qa_l)","  ","  def get_max_action(self, s):","    qa_l = self.sess.run(self.Qa_ph,","                         feed_dict={self.s_ph: [[s]] } )","    return np.argmax(qa_l)","","# #############################################  Test  ########################################### #","def test():","  s_len, a_len, nn_len = 3, 3, 10","  scher = PolicyGradLearner(s_len, a_len, nn_len)","  # scher = QLearner(s_len, a_len, nn_len)","  ","  def state():","    s = np.random.randint(10, size=s_len)","    sum_s = sum(s)","    return s/sum_s if sum_s != 0 else s","  ","  def reward(s, a):","    # s_min = min(s)","    # r = 10 if s[a] == s_min else 0","    # return min(100, 1/(0.001 + s[a] - min(s) ) )","    # return 100*math.exp(-(s[a] - min(s) ) )","    return 1/(0.1 + s[a] - min(s) )","  ","  def evaluate():","    num_shortest_found = 0","    for e in range(100):","      s = state()","      a = scher.get_max_action(s)","      if s[a] - min(s) < 0.01:","        num_shortest_found += 1","    print(\"freq shortest found= {}\".format(num_shortest_found/100) )","  ","  def train_w_mult_trajs():","    N, T = 10, 100","    def gen_N_traj():","      n_t_s_l, n_t_a_l, n_t_r_l = np.zeros((N, T, s_len)), np.zeros((N, T, 1)), np.zeros((N, T, 1))","      for n in range(N):","        for t in range(T):","          s = state()","          a = scher.get_random_action(s)","          n_t_s_l[n, t, :] = s","          n_t_a_l[n, t, :] = a","          n_t_r_l[n, t, :] = reward(s, a)","      return n_t_s_l, n_t_a_l, n_t_r_l","    ","    for i in range(100*20):","      n_t_s_l, n_t_a_l, n_t_r_l = gen_N_traj()","      scher.train_w_mult_trajs(n_t_s_l, n_t_a_l, n_t_r_l)","      if i % 10 == 0:","        evaluate()","  train_w_mult_trajs()","","def vsimple_regress():","  s_len = 3","  T = 100","  def state():","    s = np.random.randint(10, size=s_len)","    sum_s = sum(s)","    return s/sum_s if sum_s != 0 else s","  ","  def reward(s):","    return 10*max(s)","  ","  def sample_traj():","    t_s_l, t_r_l = np.zeros((T, s_len)), np.zeros((T, 1))","    for t in range(T):","      s = state()","      t_s_l[t, :] = s","      t_r_l[t, :] = reward(s)","    return t_s_l, t_r_l","  ","  value_ester = VEster(s_len, nn_len=10, straj_training=False)","  for i in range(100*40):","    t_s_l, t_r_l = sample_traj()","    value_ester.train_w_single_traj(t_s_l, t_r_l)","","class A(object):","  def __init__(self):","    self.a = 'A'","  ","  def __repr__(self):","    return self.a","  ","  def test(self):","    print(\"self.a= {}\".format(self.a) )","  ","class B(A):","  def __init__(self):","    super().__init__()","    ","    self.init()","  ","  def init(self):","    self.a = 'B'","","if __name__ == \"__main__\":","  # test()","  # vsimple_regress()","  ","  # b = B()","  # # print(\"b= {}\".format(b) )","  # b.test()","  ","  # learner = PolicyGradLearner(s_len=1, a_len=1)","  learner = QLearner(s_len=1, a_len=1)","  learner.save(0)","  restore_result = learner.restore(0)","  print(\"restore_result= {}\".format(restore_result) )"]},{"action":"insertText","range":{"start":{"row":0,"column":0},"end":{"row":0,"column":25}},"text":"import math, time, random"},{"action":"insertText","range":{"start":{"row":0,"column":25},"end":{"row":1,"column":0}},"text":"\n"},{"action":"insertLines","range":{"start":{"row":1,"column":0},"end":{"row":384,"column":0}},"lines":["import numpy as np","import tensorflow as tf","","from log_utils import *","","def rewards_to_qvals(t_r_l, gamma):","  T = t_r_l.shape[0]","  # reward = average of all following rewards","  # for t in range(T):","  #   t_r_l[t, 0] = np.mean(t_r_l[t:, 0])","  ","  # for t in range(T):","  #   cumw, cumr = 0, 0","  #   for i, r in enumerate(t_r_l[t:, 0] ):","  #     cumw += gamma**i","  #     cumr += gamma**i * r","  #   t_r_l[t, 0] = cumr/cumw","  # return t_r_l","  ","  t_dr_l = np.zeros((T, 1))","  cumw, cumr = 0, 0","  for t in range(T-1, -1, -1):","    cumr = t_r_l[t, 0] + gamma*cumr","    # cumw = 1 + gamma*cumw","    # t_dr_l[t, 0] = cumr/cumw","    t_dr_l[t, 0] = cumr","  return t_dr_l","","# #######################################  Value Estimator  ###################################### #","class VEster(object): # Value Estimator","  def __init__(self, s_len, nn_len):","    self.s_len = s_len","    self.nn_len = nn_len","    ","    self.init()","  ","  def __repr__(self):","    return \"VEster[s_len= {}]\".format(self.s_len)","  ","  def init(self):","    # N x T x s_len","    self.s_ph = tf.placeholder(shape=(None, None, self.s_len), dtype=tf.float32)","    # self.hidden1 = tf.contrib.layers.fully_connected(self.s_ph, self.nn_len, activation_fn=tf.nn.relu)","    # self.hidden2 = tf.contrib.layers.fully_connected(self.hidden1, self.nn_len, activation_fn=tf.nn.relu)","    # self.v = tf.contrib.layers.fully_connected(self.hidden2, 1, activation_fn=None)","    self.hidden = tf.contrib.layers.fully_connected(self.s_ph, self.nn_len, activation_fn=tf.nn.relu, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    self.v = tf.contrib.layers.fully_connected(self.hidden, 1, activation_fn=None, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    ","    self.sampled_v = tf.placeholder(shape=(None, None, 1), dtype=tf.float32)","    # self.loss = tf.reduce_sum(tf.squared_difference(self.v, self.sampled_v) )","    self.loss = tf.losses.mean_squared_error(self.v, self.sampled_v) + \\","      tf.losses.get_regularization_loss()","    ","    # self.optimizer = tf.train.GradientDescentOptimizer(0.01)","    self.optimizer = tf.train.AdamOptimizer(0.01)","    self.train_op = self.optimizer.minimize(self.loss)","    ","    self.sess = tf.Session()","    self.sess.run(tf.global_variables_initializer() )","  ","  def train_w_mult_trajs(self, n_t_s_l, n_t_v_l):","    _, loss = self.sess.run([self.train_op, self.loss],","                            feed_dict={self.s_ph: n_t_s_l,","                                       self.sampled_v: n_t_v_l} )","    print(\"VEster:: loss= {}\".format(loss) )","  ","  def get_v(self, n_t_s_l):","    return self.sess.run(self.v,","                         feed_dict={self.s_ph: n_t_s_l} )","","# #############################################  Learner  ###################################### #","class Learner(object):","  def __init__(self, s_len, a_len, nn_len):","    self.s_len = s_len","    self.a_len = a_len","    self.nn_len = nn_len","    ","    self.gamma = 0.99 # 0.8","    ","    self.saver = None","    self.sess = None","  ","  def save(self, step):","    save_name = 'save/{}'.format(self)","    save_path = self.saver.save(self.sess, save_name, global_step=step)","    log(WARNING, \"saved; \", save_path=save_path)","  ","  def restore(self, step):","    save_name = 'save/{}-{}'.format(self, step)","    try:","      save_path = self.saver.restore(self.sess, save_name)","      # log(WARNING, \"restored; \", save_path=save_path)","      return True","    except:","      return False","","# ####################################  Policy Gradient Learner  ################################# #","class PolicyGradLearner(Learner):","  def __init__(self, s_len, a_len, nn_len=10, w_actorcritic=False):","    super().__init__(s_len, a_len, nn_len)","    self.w_actorcritic = w_actorcritic","    ","    self.v_ester = VEster(s_len, nn_len)","    self.init()","    self.saver = tf.train.Saver(max_to_keep=5)","    ","    # self.save_name = 'save/PolicyGradLearner_gamma{}_slen{}_alen{}_nnlen{}_wactorcritic{}'.format(self.gamma, s_len, a_len, nn_len, w_actorcritic)","  ","  def __repr__(self):","    return 'PolicyGradLearner(s_len= {}, a_len= {}, nn_len= {}, gamma= {}, w_actorcritic= {})'.format(self.s_len, self.a_len, self.nn_len, self.gamma, self.w_actorcritic)","  ","  def init(self):","    # N x T x s_len","    self.s_ph = tf.placeholder(tf.float32, shape=(None, None, self.s_len) )","    hidden1 = tf.contrib.layers.fully_connected(self.s_ph, self.nn_len, activation_fn=tf.nn.relu, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    hidden2 = tf.contrib.layers.fully_connected(hidden1, self.nn_len, activation_fn=tf.nn.relu, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    self.a_probs = tf.contrib.layers.fully_connected(hidden2, self.a_len, activation_fn=tf.nn.softmax, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    # self.a_probs = tf.contrib.layers.fully_connected(hidden1, self.a_len, activation_fn=tf.nn.softmax, weights_regularizer=tf.contrib.layers.l2_regularizer(0.01) )","    ","    self.a_ph = tf.placeholder(tf.int32, shape=(None, None, 1), name='a_ph')","    self.q_ph = tf.placeholder(tf.float32, shape=(None, None, 1), name='q_ph')","    self.v_ph = tf.placeholder(tf.float32, shape=(None, None, 1), name='v_ph')","    ","    sh = tf.shape(self.a_probs)","    N, T = sh[0], sh[1]","    indices = tf.range(0, N*T)*sh[2] + tf.reshape(self.a_ph, [-1] )","    self.resp_outputs = tf.reshape(tf.gather(tf.reshape(self.a_probs, [-1] ), indices), (N, T, 1) )","    self.loss = \\","      -tf.reduce_mean(tf.reduce_sum(tf.log(self.resp_outputs)*(self.q_ph - self.v_ph), axis=1), axis=0) + \\","      tf.losses.get_regularization_loss()","    ","    self.optimizer = tf.train.AdamOptimizer(0.01) # tf.train.GradientDescentOptimizer(0.01)","    self.train_op = self.optimizer.minimize(self.loss)","    ","    self.sess = tf.Session()","    self.sess.run(tf.global_variables_initializer() )","  ","  def train_w_mult_trajs(self, n_t_s_l, n_t_a_l, n_t_r_l):","    # All trajectories use the same policy","    N = len(n_t_s_l)","    T = len(n_t_s_l[0] )","    # print(\"n_t_s_l.shape= {}\".format(n_t_s_l.shape) )","    # print(\"avg r= {}\".format(np.mean(n_t_r_l) ) )","    ","    if not self.w_actorcritic:","      n_t_q_l = np.zeros((N, T, 1))","      for n in range(N):","        n_t_q_l[n] = rewards_to_qvals(n_t_r_l[n], self.gamma)","      # print(\"n_t_q_l= {}\".format(n_t_q_l) )","      # print(\"n_t_q_l.shape= {}\".format(n_t_q_l.shape) )","      print(\"PolicyGradLearner:: avg q= {}\".format(np.mean(n_t_q_l) ) )","      ","      t_avgq_l = np.array([np.mean(n_t_q_l[:, t, 0] ) for t in range(T) ] ).reshape((T, 1))","      # m = np.mean(n_t_q_l)","      # t_avgq_l = np.array([m for t in range(T) ] ).reshape((T, 1))","      n_t_v_l = np.zeros((N, T, 1))","      for n in range(N):","        n_t_v_l[n] = t_avgq_l","      # print(\"n_t_v_l= {}\".format(n_t_v_l) )","      # print(\"n_t_v_l.shape= {}\".format(n_t_v_l.shape) )","      ","      loss, _ = self.sess.run([self.loss, self.train_op],","                              feed_dict={self.s_ph: n_t_s_l,","                                         self.a_ph: n_t_a_l,","                                         self.q_ph: n_t_q_l,","                                         self.v_ph: n_t_v_l} )","    else:","      # Policy gradient by getting baseline values from actor-critic","      n_t_v_l = np.zeros((N, T, 1))","      n_t_vest_l = self.v_ester.get_v(n_t_s_l)","      for t in range(T-1):","        n_t_v_l[:, t] = n_t_r_l[:, t] + self.gamma*n_t_vest_l[:, t+1]","      n_t_v_l[:, T-1] = n_t_r_l[:, T-1]","      self.v_ester.train_w_mult_trajs(n_t_s_l, n_t_v_l)","      ","      n_t_v_l = self.v_ester.get_v(n_t_s_l)","      n_t_q_l = np.zeros((N, T, 1))","      # for n in range(N):","      #   for t in range(T-1):","      #     n_t_q_l[n, t] = n_t_r_l[n, t] + self.gamma*n_t_v_l[n, t+1]","      #   n_t_q_l[n, T-1] = n_t_r_l[n, t]","      for t in range(T-1):","        n_t_q_l[:, t] = n_t_r_l[:, t] + self.gamma*n_t_v_l[:, t+1]","      n_t_q_l[:, T-1] = n_t_r_l[:, T-1]","      loss, _ = self.sess.run([self.loss, self.train_op],","                              feed_dict={self.s_ph: n_t_s_l,","                                         self.a_ph: n_t_a_l,","                                         self.q_ph: n_t_q_l,","                                         self.v_ph: n_t_v_l} )","    log(INFO, \"PolicyGradLearner;\", loss=loss)","  ","  def get_action_dist(self, s):","    a_probs = self.sess.run(self.a_probs, feed_dict={self.s_ph: [[s]] } )","    return np.array(a_probs[0][0] )","  ","  def get_random_action(self, s):","    a_probs = self.sess.run(self.a_probs, feed_dict={self.s_ph: [[s]] } )","    a_dist = np.array(a_probs[0][0] )","    # log(WARNING, \"\", s=s, a_dist=a_dist)","    a = np.random.choice(a_dist, 1, p=a_dist)","    a = np.argmax(a_dist == a)","    return a","  ","  def get_max_action(self, s):","    a_probs = self.sess.run(self.a_probs, feed_dict={self.s_ph: [[s]] } )","    a_dist = a_probs[0][0]","    # print(\"a_dist= {}\".format(a_dist) )","    return np.argmax(a_dist)  ","","# ###########################################  Q Learning  ####################################### #","class QLearner(Learner):","  def __init__(self, s_len, a_len, nn_len=10):","    super().__init__(s_len, a_len, nn_len)","    self.eps = 0.1","    self.init()","    self.saver = tf.train.Saver(max_to_keep=5)","    ","  def __repr__(self):","    return 'QLearner(s_len= {}, a_len= {})'.format(self.s_len, self.a_len)","  ","  def init(self):","    # N x T x s_len","    self.s_ph = tf.placeholder(tf.float32, shape=(None, None, self.s_len) )","    hidden1 = tf.contrib.layers.fully_connected(self.s_ph, self.nn_len, activation_fn=tf.nn.relu)","    hidden2 = tf.contrib.layers.fully_connected(hidden1, self.nn_len, activation_fn=tf.nn.relu)","    self.Qa_ph = tf.contrib.layers.fully_connected(hidden2, self.a_len, activation_fn=None)","    ","    self.a_ph = tf.placeholder(tf.int32, shape=(None, None, 1), name=\"a_ph\")","    self.targetq_ph = tf.placeholder(tf.float32, shape=(None, None, 1), name=\"q_ph\")","    ","    sh = tf.shape(self.Qa_ph)","    N, T = sh[0], sh[1]","    indices = tf.range(0, N*T)*sh[2] + tf.reshape(self.a_ph, [-1] )","    self.resp_outputs = tf.reshape(tf.gather(tf.reshape(self.Qa_ph, [-1] ), indices), (N, T, 1) )","    self.loss = tf.losses.mean_squared_error(self.resp_outputs, self.targetq_ph)","    ","    self.optimizer = tf.train.AdamOptimizer(0.01)","    self.train_op = self.optimizer.minimize(self.loss)","    ","    self.sess = tf.Session()","    self.sess.run(tf.global_variables_initializer() )","  ","  def train_w_mult_trajs(self, n_t_s_l, n_t_a_l, n_t_r_l):","    N = len(n_t_s_l)","    T = len(n_t_s_l[0] )","    ","    n_t_q_l = self.sess.run(self.Qa_ph,","                            feed_dict={self.s_ph: n_t_s_l} )","    n_t_targetq_l = np.zeros((N, T, 1))","    for n in range(N):","      for t in range(T):","        if t < T-1:","          n_t_targetq_l[n, t, 0] = n_t_r_l[n, t, 0] + self.gamma*max(n_t_q_l[n, t+1, :] )","        else:","          n_t_targetq_l[n, t, 0] = n_t_r_l[n, t, 0]","    ","    # n_t_targetq_l = np.zeros((N, T, 1))","    # for n in range(N):","    #   n_t_targetq_l[n] = rewards_to_qvals(n_t_r_l[n], self.gamma)","    ","    loss, _ = self.sess.run([self.loss, self.train_op],","                            feed_dict={self.s_ph: n_t_s_l,","                                       self.a_ph: n_t_a_l,","                                       self.targetq_ph: n_t_targetq_l} )","    print(\"QLearner:: loss= {}\".format(loss) )","    self.eps *= 0.99","  ","  def get_random_action(self, s):","    if random.uniform(0, 1) < self.eps:","      return np.random.randint(self.a_len, size=1)[0]","    else:","      qa_l = self.sess.run(self.Qa_ph,","                           feed_dict={self.s_ph: [[s]] } )","      return np.argmax(qa_l)","  ","  def get_max_action(self, s):","    qa_l = self.sess.run(self.Qa_ph,","                         feed_dict={self.s_ph: [[s]] } )","    return np.argmax(qa_l)","","# #############################################  Test  ########################################### #","def test():","  s_len, a_len, nn_len = 3, 3, 10","  scher = PolicyGradLearner(s_len, a_len, nn_len)","  # scher = QLearner(s_len, a_len, nn_len)","  ","  def state():","    s = np.random.randint(10, size=s_len)","    sum_s = sum(s)","    return s/sum_s if sum_s != 0 else s","  ","  def reward(s, a):","    # s_min = min(s)","    # r = 10 if s[a] == s_min else 0","    # return min(100, 1/(0.001 + s[a] - min(s) ) )","    # return 100*math.exp(-(s[a] - min(s) ) )","    return 1/(0.1 + s[a] - min(s) )","  ","  def evaluate():","    num_shortest_found = 0","    for e in range(100):","      s = state()","      a = scher.get_max_action(s)","      if s[a] - min(s) < 0.01:","        num_shortest_found += 1","    print(\"freq shortest found= {}\".format(num_shortest_found/100) )","  ","  def train_w_mult_trajs():","    N, T = 10, 100","    def gen_N_traj():","      n_t_s_l, n_t_a_l, n_t_r_l = np.zeros((N, T, s_len)), np.zeros((N, T, 1)), np.zeros((N, T, 1))","      for n in range(N):","        for t in range(T):","          s = state()","          a = scher.get_random_action(s)","          n_t_s_l[n, t, :] = s","          n_t_a_l[n, t, :] = a","          n_t_r_l[n, t, :] = reward(s, a)","      return n_t_s_l, n_t_a_l, n_t_r_l","    ","    for i in range(100*20):","      n_t_s_l, n_t_a_l, n_t_r_l = gen_N_traj()","      scher.train_w_mult_trajs(n_t_s_l, n_t_a_l, n_t_r_l)","      if i % 10 == 0:","        evaluate()","  train_w_mult_trajs()","","def vsimple_regress():","  s_len = 3","  T = 100","  def state():","    s = np.random.randint(10, size=s_len)","    sum_s = sum(s)","    return s/sum_s if sum_s != 0 else s","  ","  def reward(s):","    return 10*max(s)","  ","  def sample_traj():","    t_s_l, t_r_l = np.zeros((T, s_len)), np.zeros((T, 1))","    for t in range(T):","      s = state()","      t_s_l[t, :] = s","      t_r_l[t, :] = reward(s)","    return t_s_l, t_r_l","  ","  value_ester = VEster(s_len, nn_len=10, straj_training=False)","  for i in range(100*40):","    t_s_l, t_r_l = sample_traj()","    value_ester.train_w_single_traj(t_s_l, t_r_l)","","class A(object):","  def __init__(self):","    self.a = 'A'","  ","  def __repr__(self):","    return self.a","  ","  def test(self):","    print(\"self.a= {}\".format(self.a) )","  ","class B(A):","  def __init__(self):","    super().__init__()","    ","    self.init()","  ","  def init(self):","    self.a = 'B'","","if __name__ == \"__main__\":","  # test()","  # vsimple_regress()","  ","  # b = B()","  # # print(\"b= {}\".format(b) )","  # b.test()","  ","  # learner = PolicyGradLearner(s_len=1, a_len=1)","  learner = QLearner(s_len=1, a_len=1)","  learner.save(0)","  restore_result = learner.restore(0)","  print(\"restore_result= {}\".format(restore_result) )"]}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":266,"column":19},"end":{"row":266,"column":20}},"text":"9"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":266,"column":19},"end":{"row":266,"column":20}},"text":"5"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":78,"column":26},"end":{"row":78,"column":27}},"text":"8"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":78,"column":26},"end":{"row":78,"column":27}},"text":"9"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":78,"column":24},"end":{"row":78,"column":27}},"text":"0.9"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":78,"column":23},"end":{"row":78,"column":24}},"text":" "}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":78,"column":22},"end":{"row":78,"column":23}},"text":"#"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":78,"column":21},"end":{"row":78,"column":22}},"text":" "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":78,"column":17},"end":{"row":78,"column":20}},"text":"0.9"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":78,"column":20},"end":{"row":78,"column":21}},"text":" "}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":78,"column":21},"end":{"row":78,"column":22}},"text":"#"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":78,"column":22},"end":{"row":78,"column":23}},"text":" "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":266,"column":4},"end":{"row":266,"column":6}},"text":"# "}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":266,"column":4},"end":{"row":266,"column":6}},"text":"# "}]}]],"redo":[]},"/modeling.py":{"scrollTop":400,"scrollLeft":0,"selection":{"start":{"row":42,"column":24},"end":{"row":42,"column":24}},"lastUse":1540942509999,"undo":[[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":57,"column":9},"end":{"row":57,"column":10}},"text":"/"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":57,"column":8},"end":{"row":57,"column":9}},"text":"2"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":8},"end":{"row":57,"column":9}},"text":"1"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":9},"end":{"row":57,"column":10}},"text":" "}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":10},"end":{"row":57,"column":11}},"text":"-"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":11},"end":{"row":57,"column":12}},"text":" "}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":57,"column":8},"end":{"row":57,"column":9}},"text":"1"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":8},"end":{"row":57,"column":9}},"text":"2"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":14},"end":{"row":57,"column":15}},"text":"/"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":15},"end":{"row":57,"column":16}},"text":"2"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":57,"column":15},"end":{"row":57,"column":16}},"text":"2"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":57,"column":14},"end":{"row":57,"column":15}},"text":"/"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":57,"column":8},"end":{"row":57,"column":14}},"text":"2 - ro"},{"action":"insertText","range":{"start":{"row":57,"column":8},"end":{"row":57,"column":14}},"text":"2 - ro"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":8},"end":{"row":57,"column":14}},"text":"2 - ro"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":14},"end":{"row":57,"column":15}},"text":" "}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":15},"end":{"row":57,"column":16}},"text":"#"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":16},"end":{"row":57,"column":17}},"text":" "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":12},"end":{"row":57,"column":13}},"text":"m"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":13},"end":{"row":57,"column":14}},"text":"a"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":14},"end":{"row":57,"column":15}},"text":"t"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":15},"end":{"row":57,"column":16}},"text":"h"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":16},"end":{"row":57,"column":17}},"text":"."}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":17},"end":{"row":57,"column":18}},"text":"s"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":18},"end":{"row":57,"column":19}},"text":"q"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":19},"end":{"row":57,"column":20}},"text":"r"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":20},"end":{"row":57,"column":21}},"text":"t"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":21},"end":{"row":57,"column":22}},"text":"("}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":24},"end":{"row":57,"column":25}},"text":")"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":58,"column":22},"end":{"row":58,"column":23}},"text":"4"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":58,"column":22},"end":{"row":58,"column":23}},"text":"1"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":58,"column":23},"end":{"row":58,"column":24}},"text":"0"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":68,"column":7},"end":{"row":68,"column":10}},"text":"2/3"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":68,"column":13},"end":{"row":68,"column":14}},"text":" "}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":68,"column":14},"end":{"row":68,"column":15}},"text":"#"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":68,"column":15},"end":{"row":68,"column":16}},"text":" "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":68,"column":16},"end":{"row":68,"column":19}},"text":"2/3"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":68,"column":7},"end":{"row":68,"column":8}},"text":"1"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":68,"column":8},"end":{"row":68,"column":9}},"text":"/"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":68,"column":9},"end":{"row":68,"column":10}},"text":"2"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":58,"column":25},"end":{"row":58,"column":26}},"text":"0"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":57,"column":8},"end":{"row":57,"column":9}},"text":"2"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":8},"end":{"row":57,"column":9}},"text":"1"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":9},"end":{"row":57,"column":10}},"text":"."}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":57,"column":10},"end":{"row":57,"column":11}},"text":"5"}]}]],"redo":[]},"/learn_wmpi.py":{"scrollTop":1896,"scrollLeft":0,"selection":{"start":{"row":131,"column":51},"end":{"row":131,"column":51}},"lastUse":1540942509974,"undo":[[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":138,"column":29},"end":{"row":138,"column":30}},"text":"1"},{"action":"insertText","range":{"start":{"row":138,"column":29},"end":{"row":138,"column":30}},"text":"0"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":138,"column":30},"end":{"row":138,"column":31}},"text":"."}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":138,"column":31},"end":{"row":138,"column":32}},"text":"2"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":138,"column":31},"end":{"row":138,"column":32}},"text":"2"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":138,"column":31},"end":{"row":138,"column":32}},"text":"1"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":138,"column":31},"end":{"row":138,"column":32}},"text":"1"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":138,"column":31},"end":{"row":138,"column":32}},"text":"2"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":138,"column":31},"end":{"row":138,"column":32}},"text":"2"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":138,"column":31},"end":{"row":138,"column":32}},"text":"1"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":155,"column":18},"end":{"row":155,"column":19}},"text":"1"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":155,"column":18},"end":{"row":155,"column":19}},"text":"2"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":155,"column":18},"end":{"row":155,"column":19}},"text":"2"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":155,"column":18},"end":{"row":155,"column":19}},"text":"1"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":155,"column":20},"end":{"row":155,"column":21}},"text":"3"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":155,"column":20},"end":{"row":155,"column":21}},"text":"2"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":131,"column":4},"end":{"row":131,"column":6}},"text":"# "}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":130,"column":4},"end":{"row":130,"column":6}},"text":"# "}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":128,"column":16},"end":{"row":128,"column":17}},"text":"1"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":128,"column":16},"end":{"row":128,"column":17}},"text":"3"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":128,"column":16},"end":{"row":128,"column":17}},"text":"3"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":128,"column":16},"end":{"row":128,"column":17}},"text":"2"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":130,"column":4},"end":{"row":130,"column":6}},"text":"# "}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":131,"column":4},"end":{"row":131,"column":6}},"text":"# "}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":128,"column":16},"end":{"row":128,"column":17}},"text":"2"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":128,"column":16},"end":{"row":128,"column":17}},"text":"3"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":131,"column":43},"end":{"row":131,"column":44}},"text":"5"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":131,"column":43},"end":{"row":131,"column":44}},"text":"1"}]}]],"redo":[]},"/sim_objs.py":{"scrollTop":1491.5,"scrollLeft":0,"selection":{"start":{"row":110,"column":11},"end":{"row":110,"column":15}},"lastUse":1540930609854,"undo":[[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":103,"column":21},"end":{"row":104,"column":0}},"text":"\n"},{"action":"insertText","range":{"start":{"row":104,"column":0},"end":{"row":104,"column":4}},"text":"    "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":104,"column":4},"end":{"row":104,"column":31}},"text":"self.straggle_m['slowdown']"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":104,"column":4},"end":{"row":104,"column":5}},"text":"s"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":104,"column":5},"end":{"row":104,"column":6}},"text":"l"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":104,"column":6},"end":{"row":104,"column":7}},"text":" "}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":104,"column":7},"end":{"row":104,"column":8}},"text":"="}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":104,"column":8},"end":{"row":104,"column":9}},"text":" "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":104,"column":6},"end":{"row":104,"column":7}},"text":"_"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":104,"column":7},"end":{"row":104,"column":8}},"text":"r"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":104,"column":8},"end":{"row":104,"column":9}},"text":"v"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":106,"column":27},"end":{"row":106,"column":54}},"text":"self.straggle_m['slowdown']"},{"action":"insertText","range":{"start":{"row":106,"column":27},"end":{"row":106,"column":32}},"text":"sl_rv"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":106,"column":31},"end":{"row":106,"column":32}},"text":"v"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":106,"column":30},"end":{"row":106,"column":31}},"text":"r"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":106,"column":29},"end":{"row":106,"column":30}},"text":"_"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":104,"column":8},"end":{"row":104,"column":9}},"text":"v"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":104,"column":7},"end":{"row":104,"column":8}},"text":"r"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":104,"column":6},"end":{"row":104,"column":7}},"text":"_"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":107,"column":30},"end":{"row":107,"column":64}},"text":"self.straggle_m['straggle_dur_rv']"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":104,"column":36},"end":{"row":105,"column":0}},"text":"\n"},{"action":"insertText","range":{"start":{"row":105,"column":0},"end":{"row":105,"column":4}},"text":"    "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":105,"column":4},"end":{"row":105,"column":38}},"text":"self.straggle_m['straggle_dur_rv']"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":105,"column":4},"end":{"row":105,"column":19}},"text":"straggle_dur_rv"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":105,"column":19},"end":{"row":105,"column":20}},"text":" "}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":105,"column":20},"end":{"row":105,"column":21}},"text":"="}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":105,"column":21},"end":{"row":105,"column":22}},"text":" "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":108,"column":30},"end":{"row":108,"column":45}},"text":"straggle_dur_rv"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":110,"column":30},"end":{"row":110,"column":62}},"text":"self.straggle_m['normal_dur_rv']"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":105,"column":56},"end":{"row":106,"column":0}},"text":"\n"},{"action":"insertText","range":{"start":{"row":106,"column":0},"end":{"row":106,"column":4}},"text":"    "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":106,"column":4},"end":{"row":106,"column":36}},"text":"self.straggle_m['normal_dur_rv']"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":106,"column":4},"end":{"row":106,"column":5}},"text":"n"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":106,"column":5},"end":{"row":106,"column":6}},"text":"o"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":106,"column":6},"end":{"row":106,"column":7}},"text":"r"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":106,"column":7},"end":{"row":106,"column":8}},"text":"m"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":106,"column":8},"end":{"row":106,"column":9}},"text":"a"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":106,"column":9},"end":{"row":106,"column":10}},"text":"l"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":106,"column":10},"end":{"row":106,"column":11}},"text":"_"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":106,"column":11},"end":{"row":106,"column":12}},"text":"d"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":106,"column":12},"end":{"row":106,"column":13}},"text":"u"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":106,"column":13},"end":{"row":106,"column":14}},"text":"r"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":106,"column":14},"end":{"row":106,"column":15}},"text":"_"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":106,"column":15},"end":{"row":106,"column":16}},"text":"v"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":106,"column":16},"end":{"row":106,"column":17}},"text":" "}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":106,"column":17},"end":{"row":106,"column":18}},"text":"="}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":106,"column":18},"end":{"row":106,"column":19}},"text":" "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":111,"column":30},"end":{"row":111,"column":42}},"text":"normal_dur_v"}]}]],"redo":[]},"/rvs.py":{"scrollTop":8283.5,"scrollLeft":0,"selection":{"start":{"row":514,"column":13},"end":{"row":514,"column":17}},"lastUse":1540928758053,"undo":[[{"group":"doc","deltas":[{"action":"removeLines","range":{"start":{"row":516,"column":0},"end":{"row":517,"column":0}},"nl":"\n","lines":["  "]}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":516,"column":19},"end":{"row":517,"column":0}},"text":"\n"},{"action":"insertText","range":{"start":{"row":517,"column":0},"end":{"row":517,"column":4}},"text":"    "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":517,"column":4},"end":{"row":517,"column":23}},"text":"if Pr_X_leq_x == 0:"},{"action":"insertText","range":{"start":{"row":517,"column":23},"end":{"row":518,"column":0}},"text":"\n"},{"action":"insertLines","range":{"start":{"row":518,"column":0},"end":{"row":519,"column":0}},"lines":["    log(ERROR, \"X.cdf(x)! = 0\", X=X, x=x)"]},{"action":"insertText","range":{"start":{"row":519,"column":0},"end":{"row":519,"column":10}},"text":"    return"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":518,"column":0},"end":{"row":518,"column":2}},"text":"  "},{"action":"insertText","range":{"start":{"row":519,"column":0},"end":{"row":519,"column":2}},"text":"  "}]}],[{"group":"doc","deltas":[{"action":"removeLines","range":{"start":{"row":522,"column":0},"end":{"row":523,"column":0}},"nl":"\n","lines":["    "]}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":522,"column":11},"end":{"row":522,"column":12}},"text":"q"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":522,"column":10},"end":{"row":522,"column":11}},"text":"e"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":522,"column":9},"end":{"row":522,"column":10}},"text":"l"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":522,"column":9},"end":{"row":522,"column":10}},"text":"g"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":522,"column":12},"end":{"row":522,"column":13}},"text":" "}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":522,"column":13},"end":{"row":522,"column":14}},"text":"="}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":522,"column":14},"end":{"row":522,"column":15}},"text":" "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":522,"column":15},"end":{"row":522,"column":25}},"text":"Pr_X_leq_x"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":522,"column":15},"end":{"row":522,"column":16}},"text":"1"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":522,"column":16},"end":{"row":522,"column":17}},"text":" "}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":522,"column":17},"end":{"row":522,"column":18}},"text":"-"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":522,"column":18},"end":{"row":522,"column":19}},"text":" "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":522,"column":29},"end":{"row":523,"column":0}},"text":"\n"},{"action":"insertText","range":{"start":{"row":523,"column":0},"end":{"row":523,"column":4}},"text":"    "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":523,"column":4},"end":{"row":523,"column":23}},"text":"if Pr_X_leq_x == 0:"},{"action":"insertText","range":{"start":{"row":523,"column":23},"end":{"row":524,"column":0}},"text":"\n"},{"action":"insertLines","range":{"start":{"row":524,"column":0},"end":{"row":525,"column":0}},"lines":["    log(ERROR, \"X.cdf(x)! = 0\", X=X, x=x)"]},{"action":"insertText","range":{"start":{"row":525,"column":0},"end":{"row":525,"column":10}},"text":"    return"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":523,"column":7},"end":{"row":523,"column":17}},"text":"Pr_X_leq_x"},{"action":"insertText","range":{"start":{"row":523,"column":7},"end":{"row":523,"column":15}},"text":"Pr_X_g_x"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":524,"column":4},"end":{"row":524,"column":6}},"text":"  "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":525,"column":4},"end":{"row":525,"column":6}},"text":"  "}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":524,"column":20},"end":{"row":524,"column":23}},"text":"cdf"},{"action":"insertText","range":{"start":{"row":524,"column":20},"end":{"row":524,"column":21}},"text":"t"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":524,"column":21},"end":{"row":524,"column":22}},"text":"a"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":524,"column":22},"end":{"row":524,"column":23}},"text":"i"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":524,"column":23},"end":{"row":524,"column":24}},"text":"l"}]}],[{"group":"doc","deltas":[{"action":"removeLines","range":{"start":{"row":526,"column":0},"end":{"row":527,"column":0}},"nl":"\n","lines":["    "]}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":526,"column":20},"end":{"row":526,"column":29}},"text":"X.tail(x)"},{"action":"insertText","range":{"start":{"row":526,"column":20},"end":{"row":526,"column":28}},"text":"Pr_X_g_x"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":524,"column":27},"end":{"row":524,"column":28}},"text":"!"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":524,"column":31},"end":{"row":524,"column":32}},"text":"!"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":518,"column":26},"end":{"row":518,"column":27}},"text":"!"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":518,"column":30},"end":{"row":518,"column":31}},"text":"!"}]}]],"redo":[]},"/zedconfig.json":{"scrollTop":0,"scrollLeft":0,"selection":{"start":{"row":14,"column":18},"end":{"row":14,"column":22}},"lastUse":1540927457124,"undo":[[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":27,"column":7},"end":{"row":28,"column":0}},"text":"\n"},{"action":"insertText","range":{"start":{"row":28,"column":0},"end":{"row":28,"column":6}},"text":"      "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":28,"column":6},"end":{"row":28,"column":31}},"text":"\"Find:Find In Project\": {"},{"action":"insertText","range":{"start":{"row":28,"column":31},"end":{"row":29,"column":0}},"text":"\n"},{"action":"insertLines","range":{"start":{"row":29,"column":0},"end":{"row":35,"column":0}},"lines":["          \"mac\": \"Command-Shift-F\",","          \"win\": \"Ctrl-Shift-F\"","      },","      \"Find:All\": {","        \"mac\": \"Command-Shift-F\",","        \"win\": \"Ctrl-Shift-F\""]},{"action":"insertText","range":{"start":{"row":35,"column":0},"end":{"row":35,"column":7}},"text":"      }"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":27,"column":7},"end":{"row":27,"column":8}},"text":","}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":29,"column":32},"end":{"row":29,"column":33}},"text":"F"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":29,"column":32},"end":{"row":29,"column":33}},"text":"H"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":30,"column":29},"end":{"row":30,"column":30}},"text":"F"},{"action":"insertText","range":{"start":{"row":30,"column":29},"end":{"row":30,"column":30}},"text":"H"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":34,"column":21},"end":{"row":34,"column":26}},"text":"Shift"},{"action":"insertText","range":{"start":{"row":34,"column":21},"end":{"row":34,"column":22}},"text":"A"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":34,"column":22},"end":{"row":34,"column":23}},"text":"l"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":34,"column":23},"end":{"row":34,"column":24}},"text":"t"}]}]],"redo":[]},"/log_utils.py":{"scrollTop":0,"scrollLeft":0,"selection":{"start":{"row":0,"column":20},"end":{"row":0,"column":20}},"lastUse":1540927068668,"undo":[],"redo":[]},"/run.sh":{"scrollTop":0,"scrollLeft":0,"selection":{"start":{"row":18,"column":2},"end":{"row":18,"column":2}},"lastUse":1540925791168,"undo":[[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":14,"column":0},"end":{"row":15,"column":0}},"text":"\n"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":14,"column":0},"end":{"row":14,"column":23}},"text":"elif [ $1 = 'p' ]; then"},{"action":"insertText","range":{"start":{"row":14,"column":23},"end":{"row":15,"column":0}},"text":"\n"},{"action":"insertText","range":{"start":{"row":15,"column":0},"end":{"row":15,"column":17}},"text":"$PY plot_scher.py"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":0},"end":{"row":15,"column":2}},"text":"  "}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":14,"column":13},"end":{"row":14,"column":14}},"text":"p"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":14,"column":13},"end":{"row":14,"column":14}},"text":"m"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":15,"column":6},"end":{"row":15,"column":16}},"text":"plot_scher"},{"action":"insertText","range":{"start":{"row":15,"column":6},"end":{"row":15,"column":7}},"text":"m"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":7},"end":{"row":15,"column":8}},"text":"o"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":8},"end":{"row":15,"column":9}},"text":"d"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":9},"end":{"row":15,"column":10}},"text":"e"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":10},"end":{"row":15,"column":11}},"text":"l"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":11},"end":{"row":15,"column":12}},"text":"i"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":12},"end":{"row":15,"column":13}},"text":"n"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":13},"end":{"row":15,"column":14}},"text":"g"}]}]],"redo":[]},"/plot_scher.py":{"scrollTop":0,"scrollLeft":0,"selection":{"start":{"row":22,"column":43},"end":{"row":22,"column":43}},"lastUse":1540925394325,"undo":[[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":24,"column":97},"end":{"row":25,"column":0}},"text":"\n"},{"action":"insertText","range":{"start":{"row":25,"column":0},"end":{"row":25,"column":2}},"text":"  "}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":20,"column":8},"end":{"row":20,"column":10}},"text":"# "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":13,"column":6},"end":{"row":13,"column":8}},"text":"# "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":13,"column":84},"end":{"row":14,"column":0}},"text":"\n"},{"action":"insertText","range":{"start":{"row":14,"column":0},"end":{"row":14,"column":6}},"text":"      "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":14,"column":6},"end":{"row":14,"column":82}},"text":"for totaldemand in np.logspace(0.1, math.log10(job_totaldemand_rv.u_l), 10):"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":14,"column":72},"end":{"row":14,"column":75}},"text":"u_l"},{"action":"insertText","range":{"start":{"row":14,"column":72},"end":{"row":14,"column":73}},"text":"m"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":14,"column":73},"end":{"row":14,"column":74}},"text":"e"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":14,"column":74},"end":{"row":14,"column":75}},"text":"a"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":14,"column":75},"end":{"row":14,"column":76}},"text":"n"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":14,"column":76},"end":{"row":14,"column":77}},"text":"("}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":14,"column":77},"end":{"row":14,"column":78}},"text":")"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":14,"column":78},"end":{"row":14,"column":79}},"text":" "}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":25,"column":20},"end":{"row":25,"column":21}},"text":"0"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":25,"column":19},"end":{"row":25,"column":20}},"text":"1"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":25,"column":19},"end":{"row":25,"column":20}},"text":"5"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":14,"column":39},"end":{"row":14,"column":40}},"text":"0"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":14,"column":6},"end":{"row":14,"column":8}},"text":"# "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":14,"column":89},"end":{"row":15,"column":0}},"text":"\n"},{"action":"insertText","range":{"start":{"row":15,"column":0},"end":{"row":15,"column":6}},"text":"      "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":6},"end":{"row":15,"column":87}},"text":"for totaldemand in np.logspace(0.01, math.log10(job_totaldemand_rv.mean() ), 10):"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":15,"column":39},"end":{"row":15,"column":40}},"text":"0"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":15,"column":35},"end":{"row":15,"column":36}},"text":"e"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":15,"column":34},"end":{"row":15,"column":35}},"text":"c"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":15,"column":33},"end":{"row":15,"column":34}},"text":"a"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":15,"column":32},"end":{"row":15,"column":33}},"text":"p"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":15,"column":31},"end":{"row":15,"column":32}},"text":"s"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":15,"column":30},"end":{"row":15,"column":31}},"text":"g"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":15,"column":29},"end":{"row":15,"column":30}},"text":"o"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":29},"end":{"row":15,"column":30}},"text":"i"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":30},"end":{"row":15,"column":31}},"text":"n"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":31},"end":{"row":15,"column":32}},"text":"p"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":15,"column":31},"end":{"row":15,"column":32}},"text":"p"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":31},"end":{"row":15,"column":32}},"text":"s"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":32},"end":{"row":15,"column":33}},"text":"p"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":33},"end":{"row":15,"column":34}},"text":"a"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":34},"end":{"row":15,"column":35}},"text":"c"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":35},"end":{"row":15,"column":36}},"text":"e"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":15,"column":42},"end":{"row":15,"column":80}},"text":"math.log10(job_totaldemand_rv.mean() )"},{"action":"insertText","range":{"start":{"row":15,"column":42},"end":{"row":15,"column":43}},"text":"1"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":43},"end":{"row":15,"column":44}},"text":"0"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":44},"end":{"row":15,"column":45}},"text":"0"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":15,"column":6},"end":{"row":15,"column":8}},"text":"# "}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":13,"column":6},"end":{"row":13,"column":8}},"text":"# "}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":26,"column":19},"end":{"row":26,"column":20}},"text":"5"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":26,"column":19},"end":{"row":26,"column":20}},"text":"1"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":26,"column":20},"end":{"row":26,"column":21}},"text":"0"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":26,"column":20},"end":{"row":26,"column":21}},"text":"0"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":26,"column":19},"end":{"row":26,"column":20}},"text":"1"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":26,"column":19},"end":{"row":26,"column":20}},"text":"5"}]}]],"redo":[]},"/w_queues/deneme.py":{"scrollTop":672,"scrollLeft":0,"selection":{"start":{"row":87,"column":4},"end":{"row":87,"column":49}},"lastUse":1540925087109,"undo":[],"redo":[]},"/scheduler.py":{"scrollTop":2440,"scrollLeft":0,"selection":{"start":{"row":169,"column":0},"end":{"row":169,"column":0}},"lastUse":1540921138565,"undo":[[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":37},"end":{"row":137,"column":38}},"text":"g"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":38},"end":{"row":137,"column":39}},"text":" "}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":39},"end":{"row":137,"column":40}},"text":"t"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":40},"end":{"row":137,"column":41}},"text":"o"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":41},"end":{"row":137,"column":42}},"text":" "}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":137,"column":21},"end":{"row":137,"column":27}},"text":"yields"},{"action":"insertText","range":{"start":{"row":137,"column":21},"end":{"row":137,"column":22}},"text":"a"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":22},"end":{"row":137,"column":23}},"text":"l"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":23},"end":{"row":137,"column":24}},"text":"l"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":24},"end":{"row":137,"column":25}},"text":"o"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":25},"end":{"row":137,"column":26}},"text":"w"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":26},"end":{"row":137,"column":27}},"text":"s"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":42},"end":{"row":137,"column":43}},"text":"c"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":43},"end":{"row":137,"column":44}},"text":"o"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":44},"end":{"row":137,"column":45}},"text":"n"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":45},"end":{"row":137,"column":46}},"text":"v"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":46},"end":{"row":137,"column":47}},"text":"e"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":47},"end":{"row":137,"column":48}},"text":"r"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":48},"end":{"row":137,"column":49}},"text":"g"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":137,"column":49},"end":{"row":137,"column":50}},"text":"e"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":14},"end":{"row":143,"column":22}},"text":"slowdown"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":143,"column":22},"end":{"row":143,"column":24}},"text":"10"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":14},"end":{"row":143,"column":16}},"text":"10"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":16},"end":{"row":143,"column":17}},"text":"*"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":138,"column":4},"end":{"row":138,"column":6}},"text":"# "},{"action":"insertText","range":{"start":{"row":139,"column":4},"end":{"row":139,"column":6}},"text":"# "},{"action":"insertText","range":{"start":{"row":140,"column":4},"end":{"row":140,"column":6}},"text":"# "},{"action":"insertText","range":{"start":{"row":141,"column":4},"end":{"row":141,"column":6}},"text":"# "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":142,"column":4},"end":{"row":143,"column":0}},"text":"\n"},{"action":"insertText","range":{"start":{"row":143,"column":0},"end":{"row":143,"column":4}},"text":"    "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":142,"column":4},"end":{"row":142,"column":22}},"text":"if slowdown < 1.1:"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":142,"column":20},"end":{"row":142,"column":21}},"text":"1"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":142,"column":20},"end":{"row":142,"column":21}},"text":"5"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":142,"column":22},"end":{"row":143,"column":0}},"text":"\n"},{"action":"insertText","range":{"start":{"row":143,"column":0},"end":{"row":143,"column":6}},"text":"      "}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":6},"end":{"row":143,"column":7}},"text":"r"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":7},"end":{"row":143,"column":8}},"text":"e"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":8},"end":{"row":143,"column":9}},"text":"t"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":9},"end":{"row":143,"column":10}},"text":"u"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":10},"end":{"row":143,"column":11}},"text":"r"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":11},"end":{"row":143,"column":12}},"text":"n"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":12},"end":{"row":143,"column":13}},"text":" "}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":13},"end":{"row":143,"column":14}},"text":"1"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":14},"end":{"row":143,"column":15}},"text":"0"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":15},"end":{"row":143,"column":16}},"text":"*"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":16},"end":{"row":143,"column":17}},"text":"("}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":17},"end":{"row":143,"column":18}},"text":")"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":17},"end":{"row":143,"column":18}},"text":"1"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":18},"end":{"row":143,"column":19}},"text":" "}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":19},"end":{"row":143,"column":20}},"text":"-"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":20},"end":{"row":143,"column":21}},"text":" "}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":21},"end":{"row":143,"column":22}},"text":"1"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":22},"end":{"row":143,"column":23}},"text":"/"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":23},"end":{"row":143,"column":24}},"text":"s"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":24},"end":{"row":143,"column":25}},"text":"l"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":25},"end":{"row":143,"column":26}},"text":"o"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":26},"end":{"row":143,"column":27}},"text":"w"}]}],[{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":27},"end":{"row":143,"column":28}},"text":"d"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":28},"end":{"row":143,"column":29}},"text":"o"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":29},"end":{"row":143,"column":30}},"text":"w"}]},{"group":"doc","deltas":[{"action":"insertText","range":{"start":{"row":143,"column":30},"end":{"row":143,"column":31}},"text":"n"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":143,"column":21},"end":{"row":143,"column":22}},"text":"1"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":143,"column":20},"end":{"row":143,"column":21}},"text":" "}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":143,"column":19},"end":{"row":143,"column":20}},"text":"-"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":143,"column":18},"end":{"row":143,"column":19}},"text":" "}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":143,"column":17},"end":{"row":143,"column":18}},"text":"1"}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":143,"column":16},"end":{"row":143,"column":17}},"text":"("}]},{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":143,"column":15},"end":{"row":143,"column":16}},"text":"*"}]}],[{"group":"doc","deltas":[{"action":"removeText","range":{"start":{"row":143,"column":24},"end":{"row":143,"column":25}},"text":")"}]}]],"redo":[]},"/w_queues/mgs_wred_model.py":{"scrollTop":0,"scrollLeft":0,"selection":{"start":{"row":0,"column":0},"end":{"row":0,"column":17}},"lastUse":1540844978153,"undo":[],"redo":[]},"/mrun.sh":{"scrollTop":0,"scrollLeft":0,"selection":{"start":{"row":13,"column":10},"end":{"row":13,"column":10}},"lastUse":1540824286620,"undo":[],"redo":[]}},"window":{"width":1440,"height":877,"top":23,"left":0,"isMaximized":true}}